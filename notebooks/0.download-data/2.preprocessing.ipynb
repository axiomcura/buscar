{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e412a0f9",
   "metadata": {},
   "source": [
    "# 2. Preprocessing Data\n",
    "\n",
    "This notebook demonstrates how to preprocess single-cell profile data for downstream analysis. It covers the following steps:\n",
    "\n",
    "**Overview**\n",
    "\n",
    "- **Data Exploration**: Examining the structure and contents of the downloaded datasets\n",
    "- **Metadata Handling**: Loading experimental metadata to guide data selection and organization\n",
    "- **Feature Selection**: Applying a shared feature space for consistency across datasets\n",
    "- **Profile Concatenation**: Merging profiles from multiple experimental plates into a unified DataFrame\n",
    "- **Format Conversion**: Converting raw CSV files to Parquet format for efficient storage and access\n",
    "- **Metadata and Feature Documentation**: Saving metadata and feature information to ensure reproducibility\n",
    "\n",
    "These preprocessing steps ensure that all datasets are standardized, well-documented, and ready for comparative and integrative analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0387feba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import pathlib\n",
    "from typing import Optional\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from utils.data_utils import split_meta_and_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc33f13a",
   "metadata": {},
   "source": [
    "## Helper functions \n",
    "\n",
    "Contains helper function that pertains to this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0f8b798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_concat_profiles(\n",
    "    profile_dir: str | pathlib.Path,\n",
    "    shared_features: Optional[list[str]] = None,\n",
    "    specific_plates: Optional[list[pathlib.Path]] = None,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Load all profile files from a directory and concatenate them into a single Polars DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    profile_dir : str or pathlib.Path\n",
    "        Directory containing the profile files (.parquet).\n",
    "    shared_features : Optional[list[str]], optional\n",
    "        List of shared feature names to filter the profiles. If None, all features are loaded.\n",
    "    specific_plates : Optional[list[pathlib.Path]], optional\n",
    "        List of specific plate file paths to load. If None, all profiles in the directory are loaded.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pl.DataFrame\n",
    "        Concatenated Polars DataFrame containing all loaded profiles.\n",
    "    \"\"\"\n",
    "    # Ensure profile_dir is a pathlib.Path\n",
    "    if isinstance(profile_dir, str):\n",
    "        profile_dir = pathlib.Path(profile_dir)\n",
    "    elif not isinstance(profile_dir, pathlib.Path):\n",
    "        raise TypeError(\"profile_dir must be a string or a pathlib.Path object\")\n",
    "\n",
    "    # Validate specific_plates\n",
    "    if specific_plates is not None:\n",
    "        if not isinstance(specific_plates, list):\n",
    "            raise TypeError(\"specific_plates must be a list of pathlib.Path objects\")\n",
    "        if not all(isinstance(path, pathlib.Path) for path in specific_plates):\n",
    "            raise TypeError(\n",
    "                \"All elements in specific_plates must be pathlib.Path objects\"\n",
    "            )\n",
    "\n",
    "    def load_profile(file: pathlib.Path) -> pl.DataFrame:\n",
    "        \"\"\"internal function to load a single profile file.\"\"\"\n",
    "        profile_df = pl.read_parquet(file)\n",
    "        meta_cols, _ = split_meta_and_features(profile_df)\n",
    "        if shared_features is not None:\n",
    "            # Only select metadata and shared features\n",
    "            return profile_df.select(meta_cols + shared_features)\n",
    "        return profile_df\n",
    "\n",
    "    # Use specific_plates if provided, otherwise gather all .parquet files\n",
    "    if specific_plates is not None:\n",
    "        # Validate that all specific plate files exist\n",
    "        for plate_path in specific_plates:\n",
    "            if not plate_path.exists():\n",
    "                raise FileNotFoundError(f\"Profile file not found: {plate_path}\")\n",
    "        files_to_load = specific_plates\n",
    "    else:\n",
    "        files_to_load = list(profile_dir.glob(\"*.parquet\"))\n",
    "        if not files_to_load:\n",
    "            raise FileNotFoundError(f\"No profile files found in {profile_dir}\")\n",
    "\n",
    "    # Load and concatenate profiles\n",
    "    loaded_profiles = [load_profile(f) for f in files_to_load]\n",
    "\n",
    "    # Concatenate all loaded profiles\n",
    "    return pl.concat(loaded_profiles, rechunk=True)\n",
    "\n",
    "\n",
    "def split_data(\n",
    "    pycytominer_output: pl.DataFrame, dataset: str = \"CP_and_DP\"\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Split pycytominer output to metadata dataframe and feature values using Polars.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pycytominer_output : pl.DataFrame\n",
    "        Polars DataFrame with pycytominer output\n",
    "    dataset : str, optional\n",
    "        Which dataset features to split,\n",
    "        can be \"CP\" or \"DP\" or by default \"CP_and_DP\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pl.DataFrame\n",
    "        Polars DataFrame with metadata and selected features\n",
    "    \"\"\"\n",
    "    all_cols = pycytominer_output.columns\n",
    "\n",
    "    # Get DP, CP, or both features from all columns depending on desired dataset\n",
    "    if dataset == \"CP\":\n",
    "        feature_cols = [col for col in all_cols if \"CP__\" in col]\n",
    "    elif dataset == \"DP\":\n",
    "        feature_cols = [col for col in all_cols if \"DP__\" in col]\n",
    "    elif dataset == \"CP_and_DP\":\n",
    "        feature_cols = [col for col in all_cols if \"P__\" in col]\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Invalid dataset '{dataset}'. Choose from 'CP', 'DP', or 'CP_and_DP'.\"\n",
    "        )\n",
    "\n",
    "    # Metadata columns is all columns except feature columns\n",
    "    metadata_cols = [col for col in all_cols if \"P__\" not in col]\n",
    "\n",
    "    # Select metadata and feature columns\n",
    "    selected_cols = metadata_cols + feature_cols\n",
    "\n",
    "    return pycytominer_output.select(selected_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f8d2d9",
   "metadata": {},
   "source": [
    "Defining the input and output directories used throughout the notebook.\n",
    "\n",
    "> **Note:** The shared profiles utilized here are sourced from the [JUMP-single-cell](https://github.com/WayScience/JUMP-single-cell) repository. All preprocessing and profile generation steps are performed in that repository, and this notebook focuses on downstream analysis using the generated profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ea207e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting data directory\n",
    "data_dir = pathlib.Path(\"./data\").resolve(strict=True)\n",
    "\n",
    "# Setting profiles directory\n",
    "profiles_dir = (data_dir / \"sc-profiles\").resolve(strict=True)\n",
    "\n",
    "# Experimental metadata\n",
    "exp_metadata_path = (\n",
    "    profiles_dir / \"cpjump1\" / \"CPJUMP1-experimental-metadata.csv\"\n",
    ").resolve(strict=True)\n",
    "\n",
    "# Setting CFReT profiles directory\n",
    "cfret_profiles_dir = (profiles_dir / \"cfret\").resolve(strict=True)\n",
    "\n",
    "# Setting feature selection path\n",
    "shared_features_config_path = (\n",
    "    profiles_dir / \"cpjump1\" / \"feature_selected_sc_qc_features.json\"\n",
    ").resolve(strict=True)\n",
    "\n",
    "# setting mitocheck profiles directory\n",
    "mitocheck_profiles_dir = (profiles_dir / \"mitocheck\").resolve(strict=True)\n",
    "mitocheck_norm_profiles_dir = (mitocheck_profiles_dir / \"normalized_data\").resolve(\n",
    "    strict=True\n",
    ")\n",
    "\n",
    "# output directories\n",
    "cpjump1_output_dir = (profiles_dir / \"cpjump1\" / \"trt-profiles\").resolve()\n",
    "cpjump1_output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Make a results folder\n",
    "results_dir = pathlib.Path(\"./results\").resolve()\n",
    "results_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7168a71a",
   "metadata": {},
   "source": [
    "Create a list of paths that only points crispr treated plates and load the shared features config file that can be found in this [repo](https://github.com/WayScience/JUMP-single-cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7944fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experimental metadata\n",
    "# selecting plates that pertains to the cpjump1 CRISPR dataset\n",
    "exp_metadata = pl.read_csv(exp_metadata_path)\n",
    "crispr_plate_names = (\n",
    "    exp_metadata.select(\"Assay_Plate_Barcode\").unique().to_series().to_list()\n",
    ")\n",
    "crispr_plate_paths = [\n",
    "    (profiles_dir / \"cpjump1\" / f\"{plate}_feature_selected_sc_qc.parquet\").resolve(\n",
    "        strict=True\n",
    "    )\n",
    "    for plate in crispr_plate_names\n",
    "]\n",
    "# Load shared features\n",
    "with open(shared_features_config_path) as f:\n",
    "    loaded_shared_features = json.load(f)\n",
    "\n",
    "shared_features = loaded_shared_features[\"shared-features\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bfd5c7",
   "metadata": {},
   "source": [
    "## Preprocessing CPJUMP1 CRISPR data\n",
    "\n",
    "Using the filtered CRISPR plate file paths and shared features configuration, we load all individual profile files and concatenate them into a single comprehensive DataFrame. This step combines data from multiple experimental plates while maintaining the consistent feature space defined by the shared features list.\n",
    "\n",
    "The concatenation process ensures:\n",
    "- All profiles use the same feature set for downstream compatibility\n",
    "- Metadata columns are preserved across all plates\n",
    "- Data integrity is maintained during the merge operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6f7e08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concat profiles already exists, loading from file\n"
     ]
    }
   ],
   "source": [
    "# Loading crispr profiles with shared features and concat into a single DataFrame\n",
    "concat_output_path = cpjump1_output_dir / \"cpjump1_crispr_trt_profiles.parquet\"\n",
    "\n",
    "if concat_output_path.exists():\n",
    "    print(\"concat profiles already exists, loading from file\")\n",
    "else:\n",
    "    loaded_profiles = load_and_concat_profiles(\n",
    "        profile_dir=profiles_dir,\n",
    "        specific_plates=crispr_plate_paths,\n",
    "        shared_features=shared_features,\n",
    "    )\n",
    "\n",
    "    # Add index column\n",
    "    loaded_profiles = loaded_profiles.with_row_index(\"index\")\n",
    "\n",
    "    # Split meta and features\n",
    "    meta_cols, features_cols = split_meta_and_features(loaded_profiles)\n",
    "\n",
    "    # Saving metadata and features of the concat profile into a json file\n",
    "    meta_features_dict = {\n",
    "        \"concat-profiles\": {\n",
    "            \"meta-features\": meta_cols,\n",
    "            \"shared-features\": features_cols,\n",
    "        }\n",
    "    }\n",
    "    with open(cpjump1_output_dir / \"concat_profiles_meta_features.json\", \"w\") as f:\n",
    "        json.dump(meta_features_dict, f, indent=4)\n",
    "\n",
    "    # filter profiles that contains treatment data\n",
    "    loaded_profiles = loaded_profiles.filter(pl.col(\"Metadata_pert_type\") == \"trt\")\n",
    "\n",
    "    # save as parquet\n",
    "    loaded_profiles.write_parquet(concat_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0ba6ad",
   "metadata": {},
   "source": [
    "## Preprocessing MitoCheck Dataset\n",
    "\n",
    "This section processes the MitoCheck dataset by loading training data, positive controls, and negative controls from compressed CSV files. The data is standardized and converted to Parquet format for consistency with other datasets and improved performance.\n",
    "\n",
    "**Key preprocessing steps:**\n",
    "\n",
    "- **Loading datasets**: Reading training data, positive controls, and negative controls from compressed CSV files\n",
    "- **Control labeling**: Adding phenotypic class labels (\"poscon\" and \"negcon\") to distinguish control types\n",
    "- **Feature filtering**: Extracting only Cell Profiler (CP) features to match the CPJUMP1 dataset structure  \n",
    "- **Column standardization**: Removing \"CP__\" prefixes and ensuring consistent naming conventions\n",
    "- **Feature alignment**: Identifying shared features across all three datasets (training, positive controls, negative controls)\n",
    "- **Metadata preservation**: Maintaining consistent metadata structure across all profile types\n",
    "- **Format conversion**: Saving processed data in optimized Parquet format for efficient downstream analysis\n",
    "\n",
    "The preprocessing ensures that all MitoCheck datasets share a common feature space and are ready for comparative analysis with CPJUMP1 profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5471d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in mitocheck profiles and save as parquet\n",
    "# drop first column which is an additional index column\n",
    "mitocheck_profile = pl.read_csv(\n",
    "    mitocheck_norm_profiles_dir / \"training_data.csv.gz\",\n",
    ")\n",
    "mitocheck_profile = mitocheck_profile.select(mitocheck_profile.columns[1:])\n",
    "\n",
    "# load in the mitocheck positive controls\n",
    "mitocheck_pos_control_profiles = pl.read_csv(\n",
    "    mitocheck_norm_profiles_dir / \"positive_control_data.csv.gz\",\n",
    ")\n",
    "\n",
    "# loading in negative control profiles\n",
    "mitocheck_neg_control_profiles = pl.read_csv(\n",
    "    mitocheck_norm_profiles_dir / \"negative_control_data.csv.gz\",\n",
    ")\n",
    "\n",
    "# insert new column \"Mitocheck_Phenotypic_Class\" for both positive and negative controls\n",
    "mitocheck_neg_control_profiles = mitocheck_neg_control_profiles.with_columns(\n",
    "    pl.lit(\"negcon\").alias(\"Mitocheck_Phenotypic_Class\")\n",
    ").select([\"Mitocheck_Phenotypic_Class\"] + mitocheck_neg_control_profiles.columns)\n",
    "\n",
    "mitocheck_pos_control_profiles = mitocheck_pos_control_profiles.with_columns(\n",
    "    pl.lit(\"poscon\").alias(\"Mitocheck_Phenotypic_Class\")\n",
    ").select([\"Mitocheck_Phenotypic_Class\"] + mitocheck_pos_control_profiles.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c141af",
   "metadata": {},
   "source": [
    "Filter Cell Profiler (CP) features and preprocess columns by removing the \"CP__\" prefix to standardize feature names for downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c420c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split profiles to only retain cell profiler features\n",
    "cp_mitocheck_profile = split_data(mitocheck_profile, dataset=\"CP\")\n",
    "cp_mitocheck_neg_control_profiles = split_data(\n",
    "    mitocheck_neg_control_profiles, dataset=\"CP\"\n",
    ")\n",
    "cp_mitocheck_pos_control_profiles = split_data(\n",
    "    mitocheck_pos_control_profiles, dataset=\"CP\"\n",
    ")\n",
    "\n",
    "# rename columns to remove \"CP__\" prefix for all datasets\n",
    "datasets = [\n",
    "    cp_mitocheck_profile,\n",
    "    cp_mitocheck_neg_control_profiles,\n",
    "    cp_mitocheck_pos_control_profiles,\n",
    "]\n",
    "(\n",
    "    cp_mitocheck_profile,\n",
    "    cp_mitocheck_neg_control_profiles,\n",
    "    cp_mitocheck_pos_control_profiles,\n",
    ") = [\n",
    "    df.rename(lambda x: x.replace(\"CP__\", \"\") if \"CP__\" in x else x) for df in datasets\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ba5e19",
   "metadata": {},
   "source": [
    "Splitting the metadata and feature columns for each dataset to enable targeted downstream analysis and ensure consistent data structure across all profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9090146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# naming the metadata of mitocheck profiles\n",
    "cp_mitocheck_profile_meta = [\n",
    "    \"Mitocheck_Phenotypic_Class\",\n",
    "    \"Cell_UUID\",\n",
    "    \"Location_Center_X\",\n",
    "    \"Location_Center_Y\",\n",
    "    \"Metadata_Plate\",\n",
    "    \"Metadata_Well\",\n",
    "    \"Metadata_Frame\",\n",
    "    \"Metadata_Site\",\n",
    "    \"Metadata_Plate_Map_Name\",\n",
    "    \"Metadata_DNA\",\n",
    "    \"Metadata_Gene\",\n",
    "    \"Metadata_Gene_Replicate\",\n",
    "    \"Metadata_Object_Outline\",\n",
    "]\n",
    "cp_mitocheck_neg_control_profiles_meta = [\n",
    "    \"Mitocheck_Phenotypic_Class\",\n",
    "    \"Cell_UUID\",\n",
    "    \"Location_Center_X\",\n",
    "    \"Location_Center_Y\",\n",
    "    \"Metadata_Plate\",\n",
    "    \"Metadata_Well\",\n",
    "    \"Metadata_Frame\",\n",
    "    \"Metadata_Site\",\n",
    "    \"Metadata_Plate_Map_Name\",\n",
    "    \"Metadata_DNA\",\n",
    "    \"Metadata_Gene\",\n",
    "    \"Metadata_Gene_Replicate\",\n",
    "    \"AreaShape_Area\",\n",
    "]\n",
    "\n",
    "cp_mitocheck_pos_control_profiles_meta = [\n",
    "    \"Mitocheck_Phenotypic_Class\",\n",
    "    \"Cell_UUID\",\n",
    "    \"Location_Center_X\",\n",
    "    \"Location_Center_Y\",\n",
    "    \"Metadata_Plate\",\n",
    "    \"Metadata_Well\",\n",
    "    \"Metadata_Frame\",\n",
    "    \"Metadata_Site\",\n",
    "    \"Metadata_Plate_Map_Name\",\n",
    "    \"Metadata_DNA\",\n",
    "    \"Metadata_Gene\",\n",
    "    \"Metadata_Gene_Replicate\",\n",
    "    \"AreaShape_Area\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d7ced04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select morphology features by droping the metadata features and getting only the column names\n",
    "cp_mitocheck_profile_features = cp_mitocheck_profile.drop(\n",
    "    cp_mitocheck_profile_meta\n",
    ").columns\n",
    "cp_mitocheck_neg_control_profiles_features = cp_mitocheck_neg_control_profiles.drop(\n",
    "    cp_mitocheck_neg_control_profiles_meta\n",
    ").columns\n",
    "cp_mitocheck_pos_control_profiles_features = cp_mitocheck_pos_control_profiles.drop(\n",
    "    cp_mitocheck_pos_control_profiles_meta\n",
    ").columns\n",
    "\n",
    "\n",
    "# now find shared profiles between all feature columns\n",
    "shared_features = list(\n",
    "    set(cp_mitocheck_profile_features)\n",
    "    & set(cp_mitocheck_neg_control_profiles_features)\n",
    "    & set(cp_mitocheck_pos_control_profiles_features)\n",
    ")\n",
    "\n",
    "# now create a json file that contains the feature space configs\n",
    "mitocheck_feature_space_configs = {\n",
    "    \"shared-features\": shared_features,\n",
    "    \"negcon-meta\": cp_mitocheck_neg_control_profiles_meta,\n",
    "    \"poscon-meta\": cp_mitocheck_pos_control_profiles_meta,\n",
    "    \"training-meta\": cp_mitocheck_profile_meta,\n",
    "}\n",
    "\n",
    "with open(mitocheck_profiles_dir / \"mitocheck_feature_space_configs.json\", \"w\") as f:\n",
    "    json.dump(mitocheck_feature_space_configs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a7f6944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now convert preprocessed Mitocheck profiles to parquet files\n",
    "cp_mitocheck_profile[cp_mitocheck_profile_meta + shared_features].write_parquet(\n",
    "    mitocheck_profiles_dir / \"treated_mitocheck_cp_profiles.parquet\"\n",
    ")\n",
    "cp_mitocheck_pos_control_profiles[\n",
    "    cp_mitocheck_pos_control_profiles_meta + shared_features\n",
    "].write_parquet(mitocheck_profiles_dir / \"poscon_mitocheck_cp_profiles.parquet\")\n",
    "cp_mitocheck_neg_control_profiles[\n",
    "    cp_mitocheck_neg_control_profiles_meta + shared_features\n",
    "].write_parquet(mitocheck_profiles_dir / \"negcon_mitocheck_cp_profiles.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "buscar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
