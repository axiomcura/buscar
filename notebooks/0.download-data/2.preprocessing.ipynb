{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e412a0f9",
   "metadata": {},
   "source": [
    "# 2. Preprocessing Data\n",
    "\n",
    "This notebook demonstrates how to preprocess single-cell profile data for downstream analysis. It covers the following steps:\n",
    "\n",
    "**Overview**\n",
    "\n",
    "- **Data Exploration**: Examining the structure and contents of the downloaded datasets\n",
    "- **Metadata Handling**: Loading experimental metadata to guide data selection and organization\n",
    "- **Feature Selection**: Applying a shared feature space for consistency across datasets\n",
    "- **Profile Concatenation**: Merging profiles from multiple experimental plates into a unified DataFrame\n",
    "- **Format Conversion**: Converting raw CSV files to Parquet format for efficient storage and access\n",
    "- **Metadata and Feature Documentation**: Saving metadata and feature information to ensure reproducibility\n",
    "\n",
    "These preprocessing steps ensure that all datasets are standardized, well-documented, and ready for comparative and integrative analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0387feba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import pathlib\n",
    "from typing import Optional\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from utils.data_utils import split_meta_and_features, add_cell_id_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc33f13a",
   "metadata": {},
   "source": [
    "## Helper functions \n",
    "\n",
    "Contains helper function that pertains to this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0f8b798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_concat_profiles(\n",
    "    profile_dir: str | pathlib.Path,\n",
    "    shared_features: Optional[list[str]] = None,\n",
    "    shared_contains_meta: bool = False,\n",
    "    specific_plates: Optional[list[pathlib.Path]] = None,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Load all profile files from a directory and concatenate them into a single Polars\n",
    "    DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    profile_dir : str or pathlib.Path\n",
    "        Directory containing the profile files (.parquet).\n",
    "    shared_features : Optional[list[str]], optional\n",
    "        List of shared feature names to filter the profiles. If None, all features are\n",
    "        loaded.\n",
    "    specific_plates : Optional[list[pathlib.Path]], optional\n",
    "        List of specific plate file paths to load. If None, all profiles in the\n",
    "        directory are loaded.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pl.DataFrame\n",
    "        Concatenated Polars DataFrame containing all loaded profiles.\n",
    "    \"\"\"\n",
    "    # Ensure profile_dir is a pathlib.Path\n",
    "    if isinstance(profile_dir, str):\n",
    "        profile_dir = pathlib.Path(profile_dir)\n",
    "    elif not isinstance(profile_dir, pathlib.Path):\n",
    "        raise TypeError(\"profile_dir must be a string or a pathlib.Path object\")\n",
    "\n",
    "    # Validate specific_plates\n",
    "    if specific_plates is not None:\n",
    "        if not isinstance(specific_plates, list):\n",
    "            raise TypeError(\"specific_plates must be a list of pathlib.Path objects\")\n",
    "        if not all(isinstance(path, pathlib.Path) for path in specific_plates):\n",
    "            raise TypeError(\n",
    "                \"All elements in specific_plates must be pathlib.Path objects\"\n",
    "            )\n",
    "\n",
    "    def load_profile(profile_path: pathlib.Path) -> pl.DataFrame:\n",
    "        \"\"\"internal function to load a single profile file.\"\"\"\n",
    "\n",
    "        # load profiles\n",
    "        profile_df = pl.read_parquet(profile_path)\n",
    "\n",
    "        # print shape\n",
    "        print(f\"Loaded profile {profile_path.name} with shape {profile_df.shape}\")\n",
    "\n",
    "        # if provided shared feature list does not contain meta, split and select\n",
    "        # then get it from the profile, if it does, just select the shared features\n",
    "        # directly\n",
    "        if shared_features is not None:\n",
    "            if not shared_contains_meta:\n",
    "                meta_cols, _ = split_meta_and_features(profile_df)\n",
    "                return profile_df.select(meta_cols + shared_features)\n",
    "\n",
    "            return profile_df.select(shared_features)\n",
    "        return profile_df\n",
    "\n",
    "    # Use specific_plates if provided, otherwise gather all .parquet files\n",
    "    if specific_plates is not None:\n",
    "        # Validate that all specific plate files exist\n",
    "        for plate_path in specific_plates:\n",
    "            if not plate_path.exists():\n",
    "                raise FileNotFoundError(f\"Profile file not found: {plate_path}\")\n",
    "        files_to_load = specific_plates\n",
    "    else:\n",
    "        files_to_load = list(profile_dir.glob(\"*.parquet\"))\n",
    "        if not files_to_load:\n",
    "            raise FileNotFoundError(f\"No profile files found in {profile_dir}\")\n",
    "\n",
    "    # Load and concatenate profiles\n",
    "    loaded_profiles = [load_profile(f) for f in files_to_load]\n",
    "\n",
    "    # Concatenate all loaded profiles\n",
    "    return pl.concat(loaded_profiles, rechunk=True)\n",
    "\n",
    "\n",
    "def split_data(\n",
    "    pycytominer_output: pl.DataFrame, dataset: str = \"CP_and_DP\"\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Split pycytominer output to metadata dataframe and feature values using Polars.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pycytominer_output : pl.DataFrame\n",
    "        Polars DataFrame with pycytominer output\n",
    "    dataset : str, optional\n",
    "        Which dataset features to split,\n",
    "        can be \"CP\" or \"DP\" or by default \"CP_and_DP\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pl.DataFrame\n",
    "        Polars DataFrame with metadata and selected features\n",
    "    \"\"\"\n",
    "    all_cols = pycytominer_output.columns\n",
    "\n",
    "    # Get DP, CP, or both features from all columns depending on desired dataset\n",
    "    if dataset == \"CP\":\n",
    "        feature_cols = [col for col in all_cols if \"CP__\" in col]\n",
    "    elif dataset == \"DP\":\n",
    "        feature_cols = [col for col in all_cols if \"DP__\" in col]\n",
    "    elif dataset == \"CP_and_DP\":\n",
    "        feature_cols = [col for col in all_cols if \"P__\" in col]\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Invalid dataset '{dataset}'. Choose from 'CP', 'DP', or 'CP_and_DP'.\"\n",
    "        )\n",
    "\n",
    "    # Metadata columns is all columns except feature columns\n",
    "    metadata_cols = [col for col in all_cols if \"P__\" not in col]\n",
    "\n",
    "    # Select metadata and feature columns\n",
    "    selected_cols = metadata_cols + feature_cols\n",
    "\n",
    "    return pycytominer_output.select(selected_cols)\n",
    "\n",
    "\n",
    "def remove_feature_prefixes(df: pl.DataFrame, prefix: str = \"CP__\") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove feature prefixes from column names in a DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pl.DataFrame\n",
    "        Input DataFrame with prefixed column names\n",
    "    prefix : str, default \"CP__\"\n",
    "        Prefix to remove from column names\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pl.DataFrame\n",
    "        DataFrame with cleaned column names\n",
    "    \"\"\"\n",
    "    return df.rename(lambda x: x.replace(prefix, \"\") if prefix in x else x)\n",
    "\n",
    "\n",
    "def find_shared_features_across_parquets(\n",
    "    profile_paths: list[str | pathlib.Path],\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Finds the intersection of column names across multiple parquet files.\n",
    "\n",
    "    This function returns the list of column names that are present in every provided parquet file.\n",
    "    The order of columns is preserved from the first file. Uses LazyFrame.collect_schema().names()\n",
    "    to avoid expensive full reads and the PerformanceWarning.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    profile_paths : list of str or pathlib.Path\n",
    "        List of paths to parquet files.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of str\n",
    "        List of shared column names present in all files, in the order from the first file.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    FileNotFoundError\n",
    "        If no parquet files are provided or any file does not exist.\n",
    "    \"\"\"\n",
    "    if not profile_paths:\n",
    "        raise FileNotFoundError(\"No parquet files provided\")\n",
    "\n",
    "    # check if they are all strings if so, convert to pathlib.Path\n",
    "    if all(isinstance(p, str) for p in profile_paths):\n",
    "        profile_paths = [pathlib.Path(p) for p in profile_paths]\n",
    "\n",
    "    for p in profile_paths:\n",
    "        if not p.exists():\n",
    "            raise FileNotFoundError(f\"Profile file not found: {p}\")\n",
    "\n",
    "    # set the first file columns as the initial set\n",
    "    first_cols = pl.scan_parquet(profile_paths[0]).collect_schema().names()\n",
    "    common = set(first_cols)\n",
    "\n",
    "    # iterate through the rest of the files and find shared columns\n",
    "    # of the rest of the profiles\n",
    "    for p in profile_paths[1:]:\n",
    "        cols = pl.scan_parquet(p).collect_schema().names()\n",
    "        common &= set(cols)\n",
    "        if not common:\n",
    "            # Early exit if no shared columns remain\n",
    "            return []\n",
    "\n",
    "    # Preserve first file ordering (Meta and features order)\n",
    "    shared_features = [c for c in first_cols if c in common]\n",
    "    return shared_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f8d2d9",
   "metadata": {},
   "source": [
    "Defining the input and output directories used throughout the notebook.\n",
    "\n",
    "> **Note:** The shared profiles utilized here are sourced from the [JUMP-single-cell](https://github.com/WayScience/JUMP-single-cell) repository. All preprocessing and profile generation steps are performed in that repository, and this notebook focuses on downstream analysis using the generated profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ea207e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting data directory\n",
    "data_dir = pathlib.Path(\"./data\").resolve(strict=True)\n",
    "\n",
    "# Setting profiles directory\n",
    "profiles_dir = (data_dir / \"sc-profiles\").resolve(strict=True)\n",
    "\n",
    "# Experimental metadata\n",
    "exp_metadata_path = (\n",
    "    profiles_dir / \"cpjump1\" / \"cpjump1_compound_experimental-metadata.csv\"\n",
    ").resolve(strict=True)\n",
    "\n",
    "# Setting CFReT profiles directory\n",
    "cfret_profiles_dir = (profiles_dir / \"cfret\").resolve(strict=True)\n",
    "cfret_profiles_path = (\n",
    "    cfret_profiles_dir / \"localhost230405150001_sc_feature_selected.parquet\"\n",
    ").resolve(strict=True)\n",
    "\n",
    "# cfret-screen profiles path\n",
    "cfret_screen_profiles_path = profiles_dir / \"cfret-screen\"\n",
    "\n",
    "# Setting feature selection path\n",
    "shared_features_config_path = (\n",
    "    profiles_dir / \"cpjump1\" / \"feature_selected_sc_qc_features.json\"\n",
    ").resolve(strict=True)\n",
    "\n",
    "# setting mitocheck profiles directory\n",
    "mitocheck_dir = (profiles_dir / \"mitocheck\").resolve(strict=True)\n",
    "mitocheck_compressed_profiles_dir = (\n",
    "    profiles_dir / \"mitocheck\" / \"normalized_data\"\n",
    ").resolve(strict=True)\n",
    "\n",
    "# seting cfret-screen profiles paths\n",
    "cfret_screen_profiles_paths = [\n",
    "    path.resolve(strict=True)\n",
    "    for path in cfret_screen_profiles_path.glob(\"*_sc_feature_selected.parquet\")\n",
    "]\n",
    "\n",
    "# output directories\n",
    "cpjump1_output_dir = (profiles_dir / \"cpjump1\").resolve()\n",
    "cpjump1_output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Make a results folder\n",
    "results_dir = pathlib.Path(\"./results\").resolve()\n",
    "results_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7168a71a",
   "metadata": {},
   "source": [
    "Create a list of paths that only points compound treated plates and load the shared features config file that can be found in this [repo](https://github.com/WayScience/JUMP-single-cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7944fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experimental metadata\n",
    "# selecting plates that pertains to the cpjump1 compound dataset\n",
    "exp_metadata = pl.read_csv(exp_metadata_path)\n",
    "compound_plate_names = (\n",
    "    exp_metadata.select(\"Assay_Plate_Barcode\").unique().to_series().to_list()\n",
    ")\n",
    "compound_plate_paths = [\n",
    "    (profiles_dir / \"cpjump1\" / f\"{plate}_feature_selected_sc_qc.parquet\").resolve(\n",
    "        strict=True\n",
    "    )\n",
    "    for plate in compound_plate_names\n",
    "]\n",
    "# Load shared features\n",
    "with open(shared_features_config_path) as f:\n",
    "    loaded_shared_features = json.load(f)\n",
    "\n",
    "shared_features = loaded_shared_features[\"shared-features\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bfd5c7",
   "metadata": {},
   "source": [
    "## Preprocessing CPJUMP1 Compound data\n",
    "\n",
    "Using the filtered compound plate file paths and shared features configuration, we load all individual profile files and concatenate them into a single comprehensive DataFrame. This step combines data from multiple experimental plates while maintaining the consistent feature space defined by the shared features list.\n",
    "\n",
    "The concatenation process ensures:\n",
    "- All profiles use the same feature set for downstream compatibility\n",
    "- Metadata columns are preserved across all plates\n",
    "- Data integrity is maintained during the merge operation\n",
    "- Adding a unique cell id has column `Metadata_cell_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6f7e08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading compound profiles with shared features and concat into a single DataFrame\n",
    "concat_output_path = (\n",
    "    cpjump1_output_dir / \"cpjump1_compound_concat_profiles.parquet\"\n",
    ").resolve()\n",
    "\n",
    "# loaded and concatenated profiles\n",
    "cpjump1_profiles = load_and_concat_profiles(\n",
    "    profile_dir=profiles_dir,\n",
    "    specific_plates=compound_plate_paths,\n",
    "    shared_features=shared_features,\n",
    ")\n",
    "\n",
    "# create an index columm and unique cell ID based on features of a single profiles\n",
    "cpjump1_profiles = add_cell_id_hash(cpjump1_profiles)\n",
    "\n",
    "# Split meta and features\n",
    "meta_cols, features_cols = split_meta_and_features(cpjump1_profiles)\n",
    "\n",
    "# Saving metadata and features of the concat profile into a json file\n",
    "meta_features_dict = {\n",
    "    \"metadata-features\": meta_cols,\n",
    "    \"morphology-features\": features_cols,\n",
    "}\n",
    "with open(cpjump1_output_dir / \"concat_profiles_meta_features.json\", \"w\") as f:\n",
    "    json.dump(meta_features_dict, f, indent=4)\n",
    "\n",
    "# save as parquet with defined order of columns\n",
    "cpjump1_profiles.select(meta_cols + features_cols).write_parquet(concat_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0ba6ad",
   "metadata": {},
   "source": [
    "## Preprocessing MitoCheck Dataset\n",
    "\n",
    "This section processes the MitoCheck dataset by loading training data, positive controls, and negative controls from compressed CSV files. The data is standardized and converted to Parquet format for consistency with other datasets and improved performance.\n",
    "\n",
    "**Key preprocessing steps:**\n",
    "\n",
    "- **Loading datasets**: Reading training data, positive controls, and negative controls from compressed CSV files\n",
    "- **Control labeling**: Adding phenotypic class labels (\"poscon\" and \"negcon\") to distinguish control types\n",
    "- **Feature filtering**: Extracting only Cell Profiler (CP) features to match the CPJUMP1 dataset structure  \n",
    "- **Column standardization**: Removing \"CP__\" prefixes and ensuring consistent naming conventions\n",
    "- **Feature alignment**: Identifying shared features across all three datasets (training, positive controls, negative controls)\n",
    "- **Metadata preservation**: Maintaining consistent metadata structure across all profile types\n",
    "- **Format conversion**: Saving processed data in optimized Parquet format for efficient downstream analysis\n",
    "- **adding cell id**: adding a cell id column `Metadata_cell_id`\n",
    "\n",
    "The preprocessing ensures that all MitoCheck datasets share a common feature space and are ready for comparative analysis with CPJUMP1 profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5471d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in mitocheck profiles and save as parquet\n",
    "# drop first column which is an additional index column\n",
    "mitocheck_profile = pl.read_csv(\n",
    "    mitocheck_compressed_profiles_dir / \"training_data.csv.gz\",\n",
    ")\n",
    "mitocheck_profile = mitocheck_profile.select(mitocheck_profile.columns[1:])\n",
    "\n",
    "# load in the mitocheck positive controls\n",
    "mitocheck_pos_control_profiles = pl.read_csv(\n",
    "    mitocheck_compressed_profiles_dir / \"positive_control_data.csv.gz\",\n",
    ")\n",
    "\n",
    "# loading in negative control profiles\n",
    "mitocheck_neg_control_profiles = pl.read_csv(\n",
    "    mitocheck_compressed_profiles_dir / \"negative_control_data.csv.gz\",\n",
    ")\n",
    "\n",
    "# insert new column \"Mitocheck_Phenotypic_Class\" for both positive and negative controls\n",
    "mitocheck_neg_control_profiles = mitocheck_neg_control_profiles.with_columns(\n",
    "    pl.lit(\"negcon\").alias(\"Mitocheck_Phenotypic_Class\")\n",
    ").select([\"Mitocheck_Phenotypic_Class\"] + mitocheck_neg_control_profiles.columns)\n",
    "\n",
    "mitocheck_pos_control_profiles = mitocheck_pos_control_profiles.with_columns(\n",
    "    pl.lit(\"poscon\").alias(\"Mitocheck_Phenotypic_Class\")\n",
    ").select([\"Mitocheck_Phenotypic_Class\"] + mitocheck_pos_control_profiles.columns)\n",
    "\n",
    "\n",
    "# insert new column \"Metadata_treatment_type\" for mitocheck profiles\n",
    "mitocheck_profile = mitocheck_profile.with_columns(\n",
    "    pl.lit(\"trt\").alias(\"Metadata_treatment_type\")\n",
    ").select([\"Metadata_treatment_type\"] + mitocheck_profile.columns)\n",
    "mitocheck_neg_control_profiles = mitocheck_neg_control_profiles.with_columns(\n",
    "    pl.lit(\"negcon\").alias(\"Metadata_treatment_type\")\n",
    ").select([\"Metadata_treatment_type\"] + mitocheck_neg_control_profiles.columns)\n",
    "mitocheck_pos_control_profiles = mitocheck_pos_control_profiles.with_columns(\n",
    "    pl.lit(\"poscon\").alias(\"Metadata_treatment_type\")\n",
    ").select([\"Metadata_treatment_type\"] + mitocheck_pos_control_profiles.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c141af",
   "metadata": {},
   "source": [
    "Filter Cell Profiler (CP) features and preprocess columns by removing the \"CP__\" prefix to standardize feature names for downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c57da947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split profiles to only retain cell profiler features\n",
    "cp_mitocheck_profile = split_data(mitocheck_profile, dataset=\"CP\")\n",
    "cp_mitocheck_neg_control_profiles = split_data(\n",
    "    mitocheck_neg_control_profiles, dataset=\"CP\"\n",
    ")\n",
    "cp_mitocheck_pos_control_profiles = split_data(\n",
    "    mitocheck_pos_control_profiles, dataset=\"CP\"\n",
    ")\n",
    "# Remove \"CP__\" prefix from all datasets for standardized feature names\n",
    "cp_mitocheck_profile = remove_feature_prefixes(cp_mitocheck_profile)\n",
    "cp_mitocheck_neg_control_profiles = remove_feature_prefixes(\n",
    "    cp_mitocheck_neg_control_profiles\n",
    ")\n",
    "cp_mitocheck_pos_control_profiles = remove_feature_prefixes(\n",
    "    cp_mitocheck_pos_control_profiles\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ba5e19",
   "metadata": {},
   "source": [
    "Splitting the metadata and feature columns for each dataset to enable targeted downstream analysis and ensure consistent data structure across all profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d7ced04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually selecting metadata features that are present across all 3 profiles\n",
    "# (negcon, poscon, and training)\n",
    "mitocheck_meta_data = [\n",
    "    \"Mitocheck_Phenotypic_Class\",\n",
    "    \"Cell_UUID\",\n",
    "    \"Location_Center_X\",\n",
    "    \"Location_Center_Y\",\n",
    "    \"Metadata_Plate\",\n",
    "    \"Metadata_Well\",\n",
    "    \"Metadata_Frame\",\n",
    "    \"Metadata_Site\",\n",
    "    \"Metadata_Plate_Map_Name\",\n",
    "    \"Metadata_DNA\",\n",
    "    \"Metadata_Gene\",\n",
    "    \"Metadata_Gene_Replicate\",\n",
    "]\n",
    "\n",
    "# select morphology features by dropping the metadata features and getting only the column names\n",
    "cp_mitocheck_profile_features = cp_mitocheck_profile.drop(mitocheck_meta_data).columns\n",
    "cp_mitocheck_neg_control_profiles_features = cp_mitocheck_neg_control_profiles.drop(\n",
    "    mitocheck_meta_data\n",
    ").columns\n",
    "cp_mitocheck_pos_control_profiles_features = cp_mitocheck_pos_control_profiles.drop(\n",
    "    mitocheck_meta_data\n",
    ").columns\n",
    "\n",
    "# now find shared profiles between all feature columns\n",
    "shared_features = list(\n",
    "    set(cp_mitocheck_profile_features)\n",
    "    & set(cp_mitocheck_neg_control_profiles_features)\n",
    "    & set(cp_mitocheck_pos_control_profiles_features)\n",
    ")\n",
    "\n",
    "# create a json file that contains the feature space configs\n",
    "# this is shared across all three differe plates: traiing, negcon, and poscon\n",
    "with open(mitocheck_dir / \"mitocheck_feature_space_configs.json\", \"w\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"metadata-features\": mitocheck_meta_data,\n",
    "            \"morphology-features\": shared_features,\n",
    "        },\n",
    "        f,\n",
    "        indent=4,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42108980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create concatenated mitocheck profiles\n",
    "concat_mitocheck_profiles = (\n",
    "    # concat all mitocheck profiles with only shared features and metadata\n",
    "    pl.concat(\n",
    "        [\n",
    "            cp_mitocheck_profile.select(mitocheck_meta_data + shared_features),\n",
    "            cp_mitocheck_neg_control_profiles.select(\n",
    "                mitocheck_meta_data + shared_features\n",
    "            ),\n",
    "            cp_mitocheck_pos_control_profiles.select(\n",
    "                mitocheck_meta_data + shared_features\n",
    "            ),\n",
    "        ],\n",
    "        rechunk=True,\n",
    "    )\n",
    "    # add index and unique cell ID\n",
    "    .with_row_index(\"index\")\n",
    ")\n",
    "\n",
    "# add unique cell ID based on features of a single profiles\n",
    "concat_mitocheck_profiles = add_cell_id_hash(concat_mitocheck_profiles)\n",
    "\n",
    "# save concatenated mitocheck profiles\n",
    "concat_mitocheck_profiles.write_parquet(\n",
    "    mitocheck_dir / \"mitocheck_concat_profiles.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daf0ec7",
   "metadata": {},
   "source": [
    "## Preprocessing CFReT Dataset\n",
    "\n",
    "This section preprocesses the CFReT dataset to ensure compatibility with downstream analysis workflows.\n",
    "\n",
    "- **Unique cell identification**: Adding `Metadata_cell_id` column with unique hash values based on all profile features to enable precise cell tracking and deduplication\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1763d383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in cfret profiles and add a unique cell ID\n",
    "cfret_profiles = pl.read_parquet(cfret_profiles_path)\n",
    "\n",
    "# adding a unique cell ID based on all features\n",
    "cfret_profiles = add_cell_id_hash(cfret_profiles, force=True)\n",
    "\n",
    "# split features\n",
    "meta_cols, features_cols = split_meta_and_features(cfret_profiles)\n",
    "\n",
    "# save feature space config to json file\n",
    "with open(cfret_profiles_dir / \"cfret_feature_space_configs.json\", \"w\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"metadata-features\": meta_cols,\n",
    "            \"morphology-features\": features_cols,\n",
    "        },\n",
    "        f,\n",
    "        indent=4,\n",
    "    )\n",
    "\n",
    "# overwrite dataset with cell\n",
    "cfret_profiles.select(meta_cols + features_cols).write_parquet(cfret_profiles_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8f7f65",
   "metadata": {},
   "source": [
    "## Preprocessing CFReT Screen Dataset\n",
    "\n",
    "This section preprocesses the CFReT Screen dataset by concatenating all plate profiles into a single unified dataframe. This represents the first batch of plates, which are technical replicates containing identical treatment conditions and dosages across all plates.\n",
    "\n",
    "**Dataset characteristics:**\n",
    "- Each plate contains both positive (n=3) and negative (n=3) controls\n",
    "- All treatment plates share the same experimental conditions\n",
    "- Technical replicates is at the plate level\n",
    "\n",
    "**Preprocessing steps:**\n",
    "\n",
    "1. **Feature alignment**: Identify shared features across all CFReT Screen plates to ensure consistent feature space\n",
    "2. **Profile concatenation**: Merge all plate profiles into a single comprehensive dataframe using the shared feature set\n",
    "3. **Unique cell identification**: Add `Metadata_cell_id` column with unique hash values to enable precise single-cell tracking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83e0411f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total shared features in cfret-screen profiles: 494\n",
      "Loaded profile localhost240927060001_sc_feature_selected.parquet with shape (12397, 652)\n",
      "Loaded profile localhost240928120001_sc_feature_selected.parquet with shape (12745, 641)\n",
      "Loaded profile localhost240926150001_sc_feature_selected.parquet with shape (16566, 657)\n",
      "Loaded profile localhost240927120001_sc_feature_selected.parquet with shape (12902, 684)\n",
      "'Metadata_cell_id' column already exists in the DataFrame. Set force=True to overwrite the existing column.\n"
     ]
    }
   ],
   "source": [
    "# find shared features across cfret-screen profiles and load and concat them\n",
    "cfret_screen_shared_features = find_shared_features_across_parquets(\n",
    "    cfret_screen_profiles_paths\n",
    ")\n",
    "print(\n",
    "    \"total shared features in cfret-screen profiles:\", len(cfret_screen_shared_features)\n",
    ")\n",
    "\n",
    "cfret_screen_concat_profiles = load_and_concat_profiles(\n",
    "    profile_dir=cfret_screen_profiles_path,\n",
    "    shared_features=cfret_screen_shared_features,\n",
    "    shared_contains_meta=True,\n",
    ")\n",
    "\n",
    "# add unique cell ID as a string type\n",
    "cfret_screen_concat_profiles = cfret_screen_concat_profiles.with_columns(\n",
    "    cfret_screen_concat_profiles.hash_rows(seed=0)\n",
    "    .alias(\"Metadata_cell_id\")\n",
    "    .cast(pl.Utf8)\n",
    ")\n",
    "\n",
    "# split the metadata and features and reorganize features in the concat profile\n",
    "cfret_screen_meta_cols, cfret_screen_features_cols = split_meta_and_features(\n",
    "    cfret_screen_concat_profiles\n",
    ")\n",
    "cfret_screen_concat_profiles = cfret_screen_concat_profiles.select(\n",
    "    cfret_screen_meta_cols + cfret_screen_features_cols\n",
    ")\n",
    "\n",
    "# save feature space config to json file\n",
    "with open(cfret_profiles_dir / \"cfret_screen_feature_space_configs.json\", \"w\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"metadata-features\": cfret_screen_meta_cols,\n",
    "            \"morphology-features\": cfret_screen_features_cols,\n",
    "        },\n",
    "        f,\n",
    "        indent=4,\n",
    "    )\n",
    "\n",
    "# add cell id hash\n",
    "cfret_screen_concat_profiles = add_cell_id_hash(cfret_screen_concat_profiles)\n",
    "\n",
    "# save concatenated cfret-screen profiles\n",
    "cfret_screen_concat_profiles.write_parquet(\n",
    "    cfret_screen_profiles_path / \"cfret_screen_concat_profiles.parquet\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "buscar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
